import pickle, json, os, requests, loggingimport numpy as npimport tensorflow as tffrom time import time, sleepfrom sklearn.preprocessing import StandardScaler, MinMaxScalerfrom flask import Flask, request, jsonifyfrom threading import Threadfrom sequence_generator import TimePredictor, SequenceGeneratorfrom a2c import Actor, Criticfrom ppo import Actor as Actor_ppofrom ppo import Critic as Critic_ppofrom replay_buffer import ReplayBufferfrom matplotlib import pyplot as pp# switch off CUDAos.environ["CUDA_VISIBLE_DEVICES"]="-1"# init Flask appapp = Flask(__name__)log = logging.getLogger('werkzeug')log.disabled = True# tf graphs and sessionssequence_generation_graph = tf.Graph()sequence_generation_session = tf.compat.v1.Session(graph=sequence_generation_graph)mover_graph = tf.Graph()mover_session = tf.compat.v1.Session(graph=mover_graph)planner_graph = tf.Graph()planner_session = tf.compat.v1.Session(graph=planner_graph)# other global variablesmode = 'USER'pause = False# excavator API@app.route('/mode', methods=['GET', 'POST'])def get_mode():    # global variables    global mode, p_count, p_tic, p_episode_count, p_action, p_hole, m_lock, m_tic, m_target_count, m_count, m_episode_count, m_action, m_state, m_score, dig_angle    # GET request    if request.method == 'GET':        # GET request when under user control        if mode == 'USER':            current_dig_angle = analyze_user_input()        # GET request when under AI control        else:            current_dig_angle = dig_angle    # POST request    elif request.method == 'POST':        data = request.data.decode('utf-8')        jdata = json.loads(data)        new_mode = jdata['mode']        # switching to the user control        if new_mode == 'USER':            current_dig_angle = None        # switching to the AI control        elif new_mode.startswith('AI'):            # AI training            if new_mode.endswith('TRAIN'):                dig_angle = generate_user_input(p_count)            # AI testing            elif new_mode.endswith('TEST'):                dig_angle = analyze_user_input(fname='user_input/user_input.txt') # CHANGE THIS TO THE FILE WITH REAL USER INPUT!            # if dig angle is set            if dig_angle is not None:                current_dig_angle = dig_angle                # nulify states and actions                p_tic = time()                p_count = 0                p_episode_count = 0                p_state = None                p_action = None                p_next_state = None                p_next_action = None                p_hole = np.zeros(p_action_dims)                m_lock = False                m_tic = time()                m_target_count = 0                m_count = 0                m_episode_count = 0                m_state = None                m_action = None                m_score = np.zeros(action_dim)            # otherwise            else:                # clean previous user input                open('user_input/user_input.txt','w').close()                # do not do anything                new_mode = 'USER'                current_dig_angle = None        else:            # unknown mode            current_dig_angle = None        # update the mode        mode = new_mode    data_to_return = {'mode': mode, 'dig_angle': current_dig_angle}    return jsonify(data_to_return)def moving_average(x, step=2, window=2):    seq = []    n = x.shape[0]    for i in np.arange(0, n, step):        idx = np.arange(np.maximum(0, i - window), np.minimum(n - 1, i + window + 1))        seq.append(np.mean(x[idx, :], axis=0))    return np.vstack(seq)def generate_sequence(model, rx_start, n_points, stretch=None):    a_start = rx_start[0]    x_start = rx_start[1:]    latent = mm.transform(ss.transform(x_start.reshape(1, n_features)))    next_steps = model.predict(latent)    y = ss.inverse_transform(mm.inverse_transform(next_steps.reshape(n_points, n_features)))    seq = np.hstack([a_start * np.ones((n_points, 1)), y])    seq = moving_average(seq)    idx_s = [0, 0, np.argmin(seq[:, 3]), 0]    if stretch is not None:        for i in np.arange(1,action_dim):            mu = seq[idx_s[i], i]            si = np.std(seq[:, i])            z = (seq[:, i] - mu) / (si + 1e-6)            idx = np.where(z <= 0)[0]            seq[idx, i] = mu + (1 + stretch[i]) * si * z[idx]    seq = moving_average(seq, step=1, window=2)    return seqdef interp_x_by_r(seq_start, seq_end, n_steps):    r = np.hstack([seq_start[0], seq_end[0]])    step = (seq_end[0] - seq_start[0]) / (n_steps - 1)    r_ = seq_start[0] + step * np.arange(n_steps)    seq = np.zeros((n_steps, len(seq_start)))    seq[:, 0] = r_    for i in range(1, len(seq_start)):        x = np.hstack([seq_start[i], seq_end[i]])        seq[:, i] = np.interp(r_, r, x)    return seqdef build_trajectory(last_point, dig):    dists = [emp[0, 3] for emp in Emps]    emp = Emps[np.argmax(dists)]    emp = moving_average(emp)    trajectory = np.vstack([        last_point,        dig,        emp    ])    print(trajectory.shape)    return trajectorydef predict_time(seq):    deltas = np.abs(seq[1:, :] - seq[:-1, :])    deltas_std = ss_D.transform(deltas)    t = timer.predict(deltas_std)    t = np.clip(t, np.min(TY), 1)    return tdef test_digger():    random_last_step_idx = np.random.randint(0, len(DX))    last_step = DX[random_last_step_idx, :].reshape(1, -1)    x = ss.inverse_transform(mm.inverse_transform(last_step))    x = np.hstack([90, x.reshape(n_features)])    y0 = generate_sequence(digger, x, n_points=n_steps, stretch=None)    yd1 = generate_sequence(digger, x, n_points=n_steps, stretch = np.array([0, 1 * depth_step, 0, 0]))    ys1 = generate_sequence(digger, x, n_points=n_steps, stretch=np.array([0, 0, 1 * swing_step, 0]))    ysm1 = generate_sequence(digger, x, n_points=n_steps, stretch = np.array([0, 0, 0, 1 * grab_step]))    pp.plot(y0, '--')    pp.plot(yd1)    pp.show()    pp.plot(y0, '--')    pp.plot(ys1)    pp.show()    pp.plot(y0, '--')    pp.plot(ysm1)    pp.show()@app.route('/trajectory')def get_trajectory():    # global variables    global m_lock, p_tic, p_state, p_action, p_next_state, p_next_action, p_hole, p_count, p_episode_count, dig_angle    # request and response data    data = request.data.decode('utf-8')    jdata = json.loads(data)    x = jdata['x']    last_step = np.array(x)  # last position    score = jdata['score']   # ground taken    data_to_send = {'y': [], 't': []}    # plan the next action    if m_episode_count % 2 == 0:        print('Trajectory completed in {0} seconds\n'.format(time() - p_tic))        print('DIGGING:')        if p_action is not None:            hole_idx = np.unravel_index(p_action.argmax(), p_action_dims)            p_hole[hole_idx] = score        p_next_state = p_hole.reshape(1, 1, p_action_dim)        # predict next action        if mode == 'AI_TRAIN':            if p_count == 0:                p_next_action = np.zeros((1, p_action_dim))            else:                p_next_action = p_actor.predict(p_state)        elif mode == 'TEST':            p_next_action = p_actor.predict(p_state)        idx = np.unravel_index(p_next_action.argmax(), p_action_dims)        angle = dig_angle + p_action_vals[0][idx[0]] * p_angle_gain        stretch = np.zeros(action_dim)        for i in np.arange(1, action_dim):            stretch[i] = p_action_vals[i][idx[i]] * p_stretch_gain / p_time_dim        dig_first_step = np.hstack([angle, x[1:]])        dig = generate_sequence(digger, dig_first_step, n_points=n_steps, stretch=stretch)        full_trajectory = np.vstack([last_step, dig]) # build_trajectory(last_step, dig)        full_trajectory[-1, 1:] = dig_end_point[1:]        print('Planner action at {0}: {1} with angle delta {2} and stretch {3}'.format(p_count, idx, p_action_vals[0][idx[0]], stretch[1:]))        p_tic = time()    else:        print('EMPTYING:')        emp_first_step = np.hstack([emp_mean_angle, x[1:]])        emp = generate_sequence(emptier, emp_first_step, n_points = n_steps)        full_trajectory = np.vstack([last_step, emp])        full_trajectory[-1, 1:] = emp_end_point[1:]    full_trajectory = clip_seq(full_trajectory, rx_min, rx_max)    trajectory_time = predict_time(full_trajectory)    trajectory_time = trajectory_time.reshape(trajectory_time.shape[0])    trajectory_time = np.ones_like(trajectory_time)    m_lock = False    if p_action is not None and m_episode_count % 2 == 1:        if p_count == p_time_dim:            done = True        p_count += 1        if p_count > 1 and not p_replay_buffer_lock:            p_replay_buffer.add(                np.reshape(p_state, (1, p_action_dim)),                np.reshape(p_action, (p_action_dim,)),                score,                done,                np.reshape(p_next_state, (1, p_action_dim))            )        # check finished or not        if done:            # learn to plan            batch = p_replay_buffer.sample_batch(p_count - 1, rnd=False)            learning_thread = Thread(target=learn_plans, args=(batch,))            learning_thread.setDaemon(True)            learning_thread.start()            p_replay_buffer.clear()            # nulify state            p_hole = np.zeros(p_action_dims)            p_state = None            p_action = None            p_count = 0            # increment episode count            dig_angle = generate_user_input(p_episode_count)            p_episode_count += 1        else:            p_state = np.array(p_next_state)            p_action = np.array(p_next_action)    data_to_send['y'] = full_trajectory[1:, :].tolist()    data_to_send['t'] = trajectory_time.tolist()    return jsonify(data_to_send)@app.route('/controls')def get_controls():    # global variables    global m_lock, m_tic, m_state, m_action, m_count, m_episode_count, m_score, m_target_count    # request and response data    data = request.data.decode('utf-8')    jdata = json.loads(data)    deltas = np.mean(np.array(jdata['deltas']), axis=0)    deltas_to_next = np.mean(np.array(jdata['deltas_to_next']), axis=0)    delta_start = np.array(jdata['delta_start'])    delta_end = np.array(jdata['delta_end'])    in_target = jdata['in_target']    time_passed = jdata['time']    time_limit = jdata['time_limit']    done = jdata['done']    data_to_send = {}    # calculate score    tol = 1e-8    error_thr = 3    #if np.all(delta_end <= error_thr):    #    component_score = np.ones(action_dim)    #    if not m_lock and m_action is not None:    #        m_target_count += 1    #elif m_count == m_count_max - 1:    #    delta_start_std = np.clip(mm_d.transform(delta_start.reshape(1, action_dim)), 0, 1).reshape(action_dim)    #    delta_end_std = np.clip(mm_d.transform(delta_end.reshape(1, action_dim)), 0, 1).reshape(action_dim)    #    # component_score = np.clip((delta_start_std - delta_end_std) / (delta_start_std + tol), 0, 1)    #    component_score = delta_start_std - delta_end_std    #    print(m_state)    #    print(m_action)    #    print(component_score)    #else:    #    component_score = np.zeros(action_dim)    #pid_score = np.hstack([component_score for _ in range(pid_dim)])    if np.all(delta_end <= error_thr):        component_score = np.ones(action_dim)        if not m_lock and m_action is not None:            m_target_count += 1    else:        delta_start_std = np.clip(mm_d.transform(delta_start.reshape(1, action_dim)), 0, 1).reshape(action_dim)        delta_end_std = np.clip(mm_d.transform(delta_end.reshape(1, action_dim)), 0, 1).reshape(action_dim)        component_score = delta_start_std - delta_end_std    pid_score = np.mean(component_score) * np.ones(pid_dim * action_dim)    if not m_lock and m_action is not None:        m_score += component_score    # predict next action    m_next_state = mm_d.transform(np.vstack([deltas, deltas_to_next]))    m_next_action = m_actor.predict(np.reshape(m_next_state, (1, m_time_dim, state_dim)))    act_values = m_next_action[0].reshape(pid_dim, action_dim)    data_to_send['controls'] = act_values.tolist()    # check if action is not none and there is need for a action, i.e. deltas are greater than threshold    if m_action is not None:        if not m_lock:            m_replay_buffer.add(                m_state,                np.reshape(m_action, (pid_dim * action_dim,)),                pid_score,                done,                m_next_state            )            m_count += 1            if not m_monte_carlo:                m_episode_count += 1                batch = m_replay_buffer.sample_batch(1, rnd=False)                learning_thread = Thread(target=learn_moves, args=(batch,))                learning_thread.setDaemon(True)                learning_thread.start()                m_replay_buffer.clear()    # check finished or not    if m_count == m_count_max:        # print stats        print('During episode {0}, {1} targets have been reached in {2} steps with score: {3}'.format(m_episode_count, m_target_count, m_count, m_score))        # learn to move        if m_monte_carlo:            try:                batch = m_replay_buffer.sample_batch(m_count * 2, rnd=False)                if m_episode_count % 2 == 1:                    learning_thread = Thread(target=learn_moves, args=(batch,))                    learning_thread.setDaemon(True)                    learning_thread.start()            except Exception as e:                print(e)            if m_episode_count % 2 == 1:                m_replay_buffer.clear()            m_lock = True        # nulify state        m_state = None        m_action = None        m_score = np.zeros(action_dim)        m_count = 0        m_target_count = 0        m_episode_count += 1    elif not m_lock:        m_state = np.array(m_next_state)        m_action = np.array(m_next_action)    return jsonify(data_to_send)# Learn to movedef learn_moves(batch, n_epochs=8):    s_batch, a_batch, r_batch, t_batch, s2_batch, i_batch, w_batch = batch    if m_monte_carlo:        v_batch = m_critic.predict(s_batch)        adv_batch = np.zeros_like(v_batch)        nsteps = s_batch.shape[0]        for i in reversed(range(nsteps)):            if t_batch[i] or i == nsteps - 1:                lastgaelam = r_batch[i, :] - v_batch[i, :]                adv_batch[i, :] = lastgaelam            else:                delta = r_batch[i, :] + m_gamma * v_batch[i + 1, :] - v_batch[i, :]                lastgaelam = delta + m_gamma * m_lambda * lastgaelam                adv_batch[i, :] = lastgaelam        trg_batch = adv_batch + v_batch        # do we standardize the advantage batch?        adv_batch = StandardScaler().fit_transform(adv_batch)    else:        v_batch = m_critic.predict(s_batch)        v2_batch = m_critic.predict(s2_batch)        trg_batch = np.zeros_like(v_batch)        nsteps = s_batch.shape[0]        for i in reversed(range(nsteps)):            if t_batch[i] or i == nsteps - 1:                trg_batch[i, :] = r_batch[i, :]            else:                trg_batch[i, :] = r_batch[i, :] + m_gamma * v2_batch[i, :]        adv_batch = trg_batch - v_batch  # this is not advantages, just td error    # train    idx = np.arange(s_batch.shape[0])    for e in range(n_epochs):        np.random.shuffle(idx)        _, actor_summary = m_actor.train(s_batch[idx, :, :], a_batch[idx, :], adv_batch[idx, :])        _, critic_summary = m_critic.train(s_batch[idx, :, :], trg_batch[idx, :])    # update summary    score_summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag='Actor/Score', simple_value=np.mean(r_batch))])    cnames = ['Slew', 'Boom', 'Arm', 'Bucket']    pnames = ['P', 'I', 'D']    pid_summaries = []    for i in range(pid_dim):        for j in range(action_dim):            pid_summaries.append(tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag='Gain/{0}/{1}'.format(pnames[i], cnames[j]), simple_value=np.mean(a_batch[:, i * action_dim + j]))]))    m_writer.add_summary(score_summary, int(m_episode_count))    for pid_summary in pid_summaries:        m_writer.add_summary(pid_summary, int(m_episode_count))    m_writer.add_summary(actor_summary, int(m_episode_count))    m_writer.add_summary(critic_summary, int(m_episode_count))    m_writer.flush()    # save model    if m_episode_count % n_epochs == 0:  # this is spaghetti coding right here!        try:            with mover_graph.as_default():                saver.save(mover_session, m_model_file, write_meta_graph=False)        except Exception as e:            print(e)# Learn to plandef learn_plans(batch, n_epochs=4):    s_batch, a_batch, r_batch, t_batch, s2_batch, i_batch, w_batch = batch    v_batch = p_critic.predict(s_batch)    n_batch = p_actor.get_neglog(s_batch)    adv_batch = np.zeros_like(v_batch)    nsteps = s_batch.shape[0]    for i in reversed(range(nsteps)):        if t_batch[i] or i == nsteps - 1:            lastgaelam = r_batch[i, :] - v_batch[i, :]            adv_batch[i, :] = lastgaelam        else:            delta = r_batch[i, :] + p_gamma * v_batch[i + 1, :] - v_batch[i, :]            lastgaelam = delta + p_gamma * p_lambda * lastgaelam            adv_batch[i, :] = lastgaelam    trg_batch = adv_batch + v_batch    # do we standardize the advantage batch?    adv_batch = StandardScaler().fit_transform(adv_batch)    idx = np.arange(s_batch.shape[0])    for e in range(n_epochs):        np.random.shuffle(idx)        _, actor_summary = p_actor.train(s_batch[idx, :, :], a_batch[idx, :], adv_batch[idx, :], n_batch[idx, :])        _, critic_summary = p_critic.train(s_batch[idx, :, :], trg_batch[idx, :], v_batch)    # update summary    p_writer.add_summary(actor_summary, int(p_episode_count))    p_writer.add_summary(critic_summary, int(p_episode_count))    p_writer.flush()    # save model    if p_episode_count % n_epochs == 0:        try:            with planner_graph.as_default():                saver.save(planner_session, p_model_file, write_meta_graph=False)        except Exception as e:            print(e)# Auxiliary functionsdef get_recorded_data(level_file = 'data/level_points.pkl'):    with open(level_file, 'rb') as f:        level_points = pickle.load(f)        key_points = pickle.load(f)        key_stats = pickle.load(f)    RX_raw = []    RX_resampled = []    deltas = []    Deltas = []    Times = []    for level in level_points:        RX_raw.append(level[0])        tasks = level[2]        for task in tasks:            RX_resampled.append(task[0])            Delta = task[0][1:, :] - task[0][:-1, :]            Deltas.append(Delta)            Times.append(task[1][0] * np.ones((Delta.shape[0], 1)))    RX_raw = np.vstack(RX_raw)    RX_resampled = np.vstack(RX_resampled)    for i in range(RX_resampled.shape[0]):        deltas.append(RX_resampled[i + 1:, :] - RX_resampled[:-i - 1, :])    x_min = np.array([-90.0, 3.9024162648733514, 13.252630737652677, 16.775050853637147])    x_max = np.array([270.0, 812.0058600513476, 1011.7128949856826, 787.6024456729566])    print('\nX min: {0}'.format(x_min))    print('X max: {0}\n'.format(x_max))    ss = StandardScaler()    mm = MinMaxScaler()    X_ss = ss.fit_transform(RX_raw[:, 1:])    mm.fit(X_ss)    # find x_min and x_max for X deltas and distances    deltas = np.abs(np.vstack(deltas))    mm_d = MinMaxScaler()    mm_d.fit(deltas)    print('\nDelta min: {0}'.format(mm_d.data_min_))    print('Delta max: {0}\n'.format(mm_d.data_max_))    # generate train datasets for sequence generation and time prediction    Deltas = np.abs(np.vstack(Deltas))    ss_D = StandardScaler()    TX = ss_D.fit_transform(Deltas)    TY = np.vstack(Times)    n_levels = len(level_points)    n_points = len(level_points[0][2][0][0])    n_features = len(level_points[0][2][0][0][0]) - 1    DX = np.zeros((n_levels, n_features))    DY = np.zeros((n_levels, n_points, n_features))    EX = np.zeros((n_levels, n_features))    EY = np.zeros((n_levels, n_points, n_features))    Digs = []    Emps = []    for i in range(n_levels):        dig = mm.transform(ss.transform(level_points[i][2][0][0][:, 1:]))        DX[i, :] = dig[0, :]        DY[i, :, :] = dig        emp = mm.transform(ss.transform(level_points[i][2][1][0][:, 1:]))        EX[i, :] = emp[0, :]        EY[i, :, :] = emp        Digs.append(level_points[i][2][0][0])        Emps.append(level_points[i][2][1][0])    return TX, TY, mm_d, ss_D, DX, DY, EX, EY, ss, mm, Digs, Emps, key_points, key_stats, x_min, x_maxdef clip_seq(seq, x_min, x_max):    seq = np.clip(        seq,        np.dot(np.ones((seq.shape[0], 1)), x_min.reshape(1, seq.shape[1])),        np.dot(np.ones((seq.shape[0], 1)), x_max.reshape(1, seq.shape[1]))    )    return seqdef generate_user_input(idx):    test_angles = [65, 92.5, 120, 180]    c_idx = np.random.choice(len(test_angles)) # int(idx % len(test_angles))    dig_a = test_angles[c_idx]    return dig_adef analyze_user_input(fname='user_input/user_input.txt', a_step=0.25, a_radius=0.5, t_step=1, t_window=10, n_thr=10):    with open(fname, 'r') as f:        lines = f.readlines()    t = []    x = []    for line in lines:        try:            spl = line.split(',')            t.append(float(spl[0]))            x.append([float(item) for item in spl[1:5]])        except:            pass    t = np.array(t)    x = np.array(x)    dig_a_found = None    if len(t) >= n_thr:        d_idx = np.arange(x.shape[0])        td = t[d_idx]        xd = x[d_idx, :]        bucket_angle_dif = []        for t in np.arange(0, np.max(td), t_step):            t_idx = np.where((td >= t) & (td < t + t_window))[0]            if len(t_idx) > 0:                ttime = td[t_idx]                slew = xd[t_idx, 0]                bucket = xd[t_idx, 3]                for x in np.arange(np.min(slew), np.max(slew), a_step):                    a_idx = np.where((slew > x - a_radius) & (slew < x + a_radius))[0]                    if len(a_idx) > 0:                        bucket_angle = bucket[a_idx]                        diff = bucket_angle[-1] - bucket_angle[0]                        if diff > dig_bucket_diff:                            avg_time = np.mean(ttime[a_idx])                            bucket_angle_dif.append([x, avg_time])        if len(bucket_angle_dif) > 0:            bucket_angle_dif = np.array(bucket_angle_dif)            dig_a_found = bucket_angle_dif[np.argmax(bucket_angle_dif[:, 1]), 0]    return dig_a_founddef switch_status():    while True:        try:            requests.post('http://127.0.0.1:5000/mode', json={'mode': 'AI_TRAIN'})            r = requests.get('http://127.0.0.1:5000/mode')            print(r.json())            print('\n******** READY! ********\n')            break        except:            pass        sleep(3)def clean_logs(log_dir):    for the_file in os.listdir(log_dir):        file_path = os.path.join(log_dir, the_file)        try:            if os.path.isfile(file_path):                os.unlink(file_path)        except Exception as e:            print(e)if __name__ == '__main__':    # sequence generator model file    ed_model_file = 'models/trajectory_generator/dense'    # train data    TX, TY, mm_d, ss_D, DX, DY, EX, EY, ss, mm, Digs, Emps, key_points, key_stats, rx_min, rx_max = get_recorded_data()    cycle_start_point, cycle_end_point, dig_start_point, dig_end_point, emp_start_point, emp_end_point = key_points    dig_bucket_diff, emp_bucket_diff, dig_mean_angle, emp_mean_angle = key_stats    n_levels = DY.shape[0]    n_steps = DY.shape[1]    n_features = DY.shape[2]    # timer, digger and emptier models    timer = TimePredictor(        sequence_generation_graph,        sequence_generation_session,        n_features + 1,        lr = 0.00001    )    digger = SequenceGenerator(        sequence_generation_graph,        sequence_generation_session,        n_features,        n_steps,        lr = 0.00001    )    emptier = SequenceGenerator(        sequence_generation_graph,        sequence_generation_session,        n_features,        n_steps,        lr=0.00001    )    with sequence_generation_graph.as_default():        saver = tf.compat.v1.train.Saver()        try:            saver.restore(sequence_generation_session, ed_model_file)        except Exception as e:            print(e)            sequence_generation_session.run(tf.compat.v1.global_variables_initializer())            timer.train(TX, TY, epochs=100000)            digger.train(DX, DY, epochs=100000)            emptier.train(EX, EY, epochs=100000)            saver.save(sequence_generation_session, ed_model_file, write_meta_graph=False)    # RL agent dimensions    state_dim = 4    action_dim = 4    pid_dim = 3    action_bound = 1    # Actor-critic model for moving    m_time_dim = 2    m_count_max = 16    m_tic = time()    m_lock = False    m_target_count = 0    m_episode_count = 0    m_count = 0    m_state = None    m_action = None    m_score = np.zeros(action_dim)    m_actor_lr = 0.001    m_critic_lr = 0.01    m_gamma = 0.99    m_lambda = 0.95    m_monte_carlo = True    m_model_file = 'models/mover/a2c'    m_summary_dir = 'logs/mover/'    clean_logs(m_summary_dir)    # episode cache    m_replay_buffer = ReplayBuffer(10000)    m_replay_buffer.clear()    # model    m_actor = Actor(        mover_graph,        mover_session,        (m_time_dim, state_dim),        action_dim * pid_dim,        action_bound,        m_actor_lr    )    m_critic = Critic(        mover_graph,        mover_session,        (m_time_dim, state_dim),        action_dim * pid_dim,        m_critic_lr    )    with mover_graph.as_default():        m_saver = tf.compat.v1.train.Saver()        try:            m_saver.restore(mover_session, m_model_file)        except Exception as e:            print(e)            mover_session.run(tf.compat.v1.global_variables_initializer())        m_writer = tf.compat.v1.summary.FileWriter(m_summary_dir, mover_session.graph)    # DDQN model for planning    p_state = None    p_action = None    p_next_state = None    p_next_action = None    p_episode_count = 0    p_count = 0    p_tic = time()    p_actor_lr = 0.01    p_critic_lr = 0.01    p_gamma = 0.99    p_lambda = 0.95    p_model_file = 'models/planner/ppo'    p_summary_dir = 'logs/planner/'    clean_logs(p_summary_dir)        # clean previous user input    open('user_input/user_input.txt','w').close()    # planner action hole    p_time_dim = 8    p_angle_gain = 7.5    p_stretch_gain = 4    depth_step = 1.0 / p_time_dim    swing_step = 1.0 / p_time_dim    grab_step = 1.0 / p_time_dim    p_action_vals = [        np.array([0, -1, 1]),        np.array([1, 2, 3, 4]),        np.array([1, 2, 3, 4]),        np.array([1, 2, 3, 4])    ]    p_action_dims = [len(vals) for vals in p_action_vals]    p_action_dim = np.prod(p_action_dims)    p_hole = np.zeros(p_action_dims)    p_actor = Actor_ppo(        planner_graph,        planner_session,        (1, p_action_dim),        p_action_dim,        p_actor_lr,        0.2    )    p_critic = Critic_ppo(        planner_graph,        planner_session,        (1, p_action_dim),        p_action_dim,        p_critic_lr,        0.2    )    with planner_graph.as_default():        p_saver = tf.compat.v1.train.Saver()        try:            p_saver.restore(planner_session, p_model_file)        except Exception as e:            print(e)            planner_session.run(tf.compat.v1.global_variables_initializer())        p_writer = tf.compat.v1.summary.FileWriter(p_summary_dir, planner_session.graph)    p_replay_buffer = ReplayBuffer(10000)    p_replay_buffer_lock = True ### CHANGE TO TRUE TO FREEZE THE PLANNER MODEL    # server    sst = Thread(target=switch_status)    sst.setDaemon(True)    sst.start()    app.run(host='0.0.0.0')